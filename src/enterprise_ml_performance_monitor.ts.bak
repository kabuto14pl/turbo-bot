/**
 * Enterprise ML Performance Monitor
 * 
 * Zaawansowany system monitorowania wydajno≈õci ML z real-time metrykami,
 * model drift detection, performance analytics i automated alerting.
 * 
 * Features:
 * - Real-time inference latency tracking
 * - Model accuracy monitoring
 * - Memory usage optimization
 * - Prediction confidence analysis
 * - Feature importance tracking
 * - Model drift detection
 * - Performance degradation alerts
 */

import * as tf from '@tensorflow/tfjs-node';
import { performance } from 'perf_hooks';
import { EventEmitter } from 'events';

interface MLPerformanceMetrics {
    timestamp: number;
    inferenceLatencyMs: number;
    memoryUsageMB: number;
    throughputPredictionsPerSecond: number;
    predictionConfidence: number;
    modelAccuracy?: number;
    featureImportance?: Record<string, number>;
    driftScore?: number;
    cpuUtilization?: number;
    gpuUtilization?: number;
}

interface ModelDriftMetrics {
    featureDrift: Record<string, number>;
    targetDrift: number;
    populationStabilityIndex: number;
    characteristicStability: number;
    timestamp: number;
}

interface PerformanceBenchmark {
    modelName: string;
    baselineLatencyMs: number;
    targetLatencyMs: number;
    baselineAccuracy: number;
    targetAccuracy: number;
    baselineMemoryMB: number;
    targetMemoryMB: number;
}

interface PredictionBatch {
    predictions: number[];
    confidenceScores: number[];
    features: number[][];
    timestamp: number;
    latencyMs: number;
}

export class EnterpriseMLPerformanceMonitor extends EventEmitter {
    private static instance: EnterpriseMLPerformanceMonitor;
    private instanceId: string;
    private isInitialized: boolean = false;
    
    // Performance tracking
    private metricsHistory: MLPerformanceMetrics[] = [];
    private driftHistory: ModelDriftMetrics[] = [];
    private predictionBatches: PredictionBatch[] = [];
    
    // Benchmarks and thresholds
    private benchmarks: Map<string, PerformanceBenchmark> = new Map();
    private alertThresholds = {
        maxLatencyMs: 100,
        minAccuracy: 0.65,
        maxMemoryMB: 512,
        maxDriftScore: 0.3,
        minConfidence: 0.6
    };
    
    // Real-time tracking
    private activeInferences: Map<string, number> = new Map();
    private rollingAverages = {
        latency: [] as number[],
        confidence: [] as number[],
        accuracy: [] as number[],
        windowSize: 100
    };
    
    // Feature analysis
    private featureStatistics: Map<string, {
        mean: number;
        std: number;
        min: number;
        max: number;
        samples: number[];
    }> = new Map();
    
    private constructor() {
        super();
        this.instanceId = `mlpm-${Date.now()}-${Math.random().toString(36).substring(2, 11)}`;
        this.setupDefaultBenchmarks();
        this.startPerformanceTracking();
    }
    
    public static getInstance(): EnterpriseMLPerformanceMonitor {
        if (!EnterpriseMLPerformanceMonitor.instance) {
            EnterpriseMLPerformanceMonitor.instance = new EnterpriseMLPerformanceMonitor();
        }
        return EnterpriseMLPerformanceMonitor.instance;
    }
    
    private setupDefaultBenchmarks(): void {
        // Policy Network benchmark
        this.benchmarks.set('PolicyNetwork', {
            modelName: 'PolicyNetwork',
            baselineLatencyMs: 50,
            targetLatencyMs: 25,
            baselineAccuracy: 0.7,
            targetAccuracy: 0.85,
            baselineMemoryMB: 64,
            targetMemoryMB: 32
        });
        
        // Value Network benchmark
        this.benchmarks.set('ValueNetwork', {
            modelName: 'ValueNetwork',
            baselineLatencyMs: 30,
            targetLatencyMs: 15,
            baselineAccuracy: 0.75,
            targetAccuracy: 0.9,
            baselineMemoryMB: 48,
            targetMemoryMB: 24
        });
        
        console.log(`[INFO] üìä ML Performance benchmarks configured for ${this.benchmarks.size} models`);
    }
    
    private startPerformanceTracking(): void {
        // Collect metrics every 10 seconds
        setInterval(() => {
            this.collectSystemMetrics();
        }, 10000);
        
        // Analyze drift every minute
        setInterval(() => {
            this.analyzeModelDrift();
        }, 60000);
        
        // Performance health check every 30 seconds
        setInterval(() => {
            this.performHealthCheck();
        }, 30000);
        
        console.log(`[INFO] üîÑ Real-time ML performance tracking started: ${this.instanceId}`);
    }
    
    public async startInferenceTracking(inferenceId: string): Promise<void> {
        this.activeInferences.set(inferenceId, performance.now());
    }
    
    public async endInferenceTracking(
        inferenceId: string,
        predictions: number[],
        confidenceScores: number[],
        features: number[][],
        modelName?: string
    ): Promise<MLPerformanceMetrics> {
        const startTime = this.activeInferences.get(inferenceId);
        if (!startTime) {
            throw new Error(`Inference ${inferenceId} not found in active tracking`);
        }
        
        const endTime = performance.now();
        const latencyMs = endTime - startTime;
        
        // Calculate metrics
        const avgConfidence = confidenceScores.reduce((a, b) => a + b, 0) / confidenceScores.length;
        const memoryUsage = this.getMemoryUsage();
        const throughput = predictions.length / (latencyMs / 1000);
        
        const metrics: MLPerformanceMetrics = {
            timestamp: Date.now(),
            inferenceLatencyMs: latencyMs,
            memoryUsageMB: memoryUsage,
            throughputPredictionsPerSecond: throughput,
            predictionConfidence: avgConfidence,
            cpuUtilization: await this.getCPUUtilization(),
            gpuUtilization: await this.getGPUUtilization()
        };
        
        // Store prediction batch
        const batch: PredictionBatch = {
            predictions,
            confidenceScores,
            features,
            timestamp: Date.now(),
            latencyMs
        };
        
        this.predictionBatches.push(batch);
        this.metricsHistory.push(metrics);
        
        // Maintain rolling window
        if (this.predictionBatches.length > 1000) {
            this.predictionBatches = this.predictionBatches.slice(-500);
        }
        if (this.metricsHistory.length > 1000) {
            this.metricsHistory = this.metricsHistory.slice(-500);
        }
        
        // Update rolling averages
        this.updateRollingAverages(metrics);
        
        // Check for performance alerts
        this.checkPerformanceAlerts(metrics, modelName);
        
        // Clean up
        this.activeInferences.delete(inferenceId);
        
        console.log(`[INFO] üìä Inference ${inferenceId}: ${latencyMs.toFixed(2)}ms, confidence: ${avgConfidence.toFixed(3)}`);
        
        return metrics;
    }
    
    private updateRollingAverages(metrics: MLPerformanceMetrics): void {
        const { windowSize } = this.rollingAverages;
        
        this.rollingAverages.latency.push(metrics.inferenceLatencyMs);
        this.rollingAverages.confidence.push(metrics.predictionConfidence);
        
        if (metrics.modelAccuracy !== undefined) {
            this.rollingAverages.accuracy.push(metrics.modelAccuracy);
        }
        
        // Maintain window size
        if (this.rollingAverages.latency.length > windowSize) {
            this.rollingAverages.latency = this.rollingAverages.latency.slice(-windowSize);
        }
        if (this.rollingAverages.confidence.length > windowSize) {
            this.rollingAverages.confidence = this.rollingAverages.confidence.slice(-windowSize);
        }
        if (this.rollingAverages.accuracy.length > windowSize) {
            this.rollingAverages.accuracy = this.rollingAverages.accuracy.slice(-windowSize);
        }
    }
    
    private checkPerformanceAlerts(metrics: MLPerformanceMetrics, modelName?: string): void {
        const alerts: string[] = [];
        
        // Latency alert
        if (metrics.inferenceLatencyMs > this.alertThresholds.maxLatencyMs) {
            alerts.push(`High latency: ${metrics.inferenceLatencyMs.toFixed(2)}ms > ${this.alertThresholds.maxLatencyMs}ms`);
        }
        
        // Memory alert
        if (metrics.memoryUsageMB > this.alertThresholds.maxMemoryMB) {
            alerts.push(`High memory usage: ${metrics.memoryUsageMB.toFixed(2)}MB > ${this.alertThresholds.maxMemoryMB}MB`);
        }
        
        // Confidence alert
        if (metrics.predictionConfidence < this.alertThresholds.minConfidence) {
            alerts.push(`Low confidence: ${metrics.predictionConfidence.toFixed(3)} < ${this.alertThresholds.minConfidence}`);
        }
        
        // Accuracy alert
        if (metrics.modelAccuracy !== undefined && metrics.modelAccuracy < this.alertThresholds.minAccuracy) {
            alerts.push(`Low accuracy: ${metrics.modelAccuracy.toFixed(3)} < ${this.alertThresholds.minAccuracy}`);
        }
        
        if (alerts.length > 0) {
            const alertData = {
                modelName: modelName || 'Unknown',
                metrics,
                alerts,
                timestamp: Date.now()
            };
            
            this.emit('performance-alert', alertData);
            console.log(`[WARN] ‚ö†Ô∏è  Performance Alert [${modelName}]:`, alerts.join(', '));
        }
    }
    
    public async analyzeModelDrift(): Promise<ModelDriftMetrics | null> {
        if (this.predictionBatches.length < 2) {
            return null;
        }
        
        const recentBatches = this.predictionBatches.slice(-50);
        const oldBatches = this.predictionBatches.slice(-100, -50);
        
        if (oldBatches.length === 0) {
            return null;
        }
        
        const featureDrift = await this.calculateFeatureDrift(recentBatches, oldBatches);
        const targetDrift = this.calculateTargetDrift(recentBatches, oldBatches);
        const psi = this.calculatePopulationStabilityIndex(recentBatches, oldBatches);
        
        const driftMetrics: ModelDriftMetrics = {
            featureDrift,
            targetDrift,
            populationStabilityIndex: psi,
            characteristicStability: 1 - psi, // Inverse relationship
            timestamp: Date.now()
        };
        
        this.driftHistory.push(driftMetrics);
        
        // Maintain drift history
        if (this.driftHistory.length > 100) {
            this.driftHistory = this.driftHistory.slice(-50);
        }
        
        // Check for drift alerts
        if (psi > this.alertThresholds.maxDriftScore) {
            this.emit('model-drift-alert', driftMetrics);
            console.log(`[WARN] üö® Model drift detected: PSI = ${psi.toFixed(4)}`);
        }
        
        console.log(`[INFO] üìà Model drift analysis: PSI = ${psi.toFixed(4)}, Target drift = ${targetDrift.toFixed(4)}`);
        
        return driftMetrics;
    }
    
    private async calculateFeatureDrift(
        recentBatches: PredictionBatch[],
        oldBatches: PredictionBatch[]
    ): Promise<Record<string, number>> {
        const featureDrift: Record<string, number> = {};
        
        if (recentBatches.length === 0 || oldBatches.length === 0) {
            return featureDrift;
        }
        
        const numFeatures = recentBatches[0].features[0]?.length || 0;
        
        for (let i = 0; i < numFeatures; i++) {
            const recentValues = recentBatches.flatMap(batch => 
                batch.features.map(features => features[i])
            ).filter(val => val !== undefined);
            
            const oldValues = oldBatches.flatMap(batch => 
                batch.features.map(features => features[i])
            ).filter(val => val !== undefined);
            
            if (recentValues.length > 0 && oldValues.length > 0) {
                const drift = this.calculateKLDivergence(recentValues, oldValues);
                featureDrift[`feature_${i}`] = drift;
            }
        }
        
        return featureDrift;
    }
    
    private calculateTargetDrift(
        recentBatches: PredictionBatch[],
        oldBatches: PredictionBatch[]
    ): number {
        const recentPredictions = recentBatches.flatMap(batch => batch.predictions);
        const oldPredictions = oldBatches.flatMap(batch => batch.predictions);
        
        if (recentPredictions.length === 0 || oldPredictions.length === 0) {
            return 0;
        }
        
        return this.calculateKLDivergence(recentPredictions, oldPredictions);
    }
    
    private calculateKLDivergence(data1: number[], data2: number[]): number {
        if (data1.length === 0 || data2.length === 0) return 0;
        
        // Create histograms
        const bins = 20;
        const min = Math.min(...data1, ...data2);
        const max = Math.max(...data1, ...data2);
        const binWidth = (max - min) / bins;
        
        const hist1 = new Array(bins).fill(0);
        const hist2 = new Array(bins).fill(0);
        
        // Fill histograms
        data1.forEach(val => {
            const bin = Math.min(Math.floor((val - min) / binWidth), bins - 1);
            hist1[bin]++;
        });
        
        data2.forEach(val => {
            const bin = Math.min(Math.floor((val - min) / binWidth), bins - 1);
            hist2[bin]++;
        });
        
        // Normalize to probabilities
        const sum1 = hist1.reduce((a, b) => a + b, 0);
        const sum2 = hist2.reduce((a, b) => a + b, 0);
        
        if (sum1 === 0 || sum2 === 0) return 0;
        
        const p1 = hist1.map(count => (count + 1e-10) / sum1); // Add smoothing
        const p2 = hist2.map(count => (count + 1e-10) / sum2);
        
        // Calculate KL divergence
        let kl = 0;
        for (let i = 0; i < bins; i++) {
            if (p1[i] > 0 && p2[i] > 0) {
                kl += p1[i] * Math.log(p1[i] / p2[i]);
            }
        }
        
        return Math.max(0, kl);
    }
    
    private calculatePopulationStabilityIndex(
        recentBatches: PredictionBatch[],
        oldBatches: PredictionBatch[]
    ): number {
        const recentPredictions = recentBatches.flatMap(batch => batch.predictions);
        const oldPredictions = oldBatches.flatMap(batch => batch.predictions);
        
        if (recentPredictions.length === 0 || oldPredictions.length === 0) {
            return 0;
        }
        
        // Create quantile-based bins
        const bins = 10;
        const oldSorted = [...oldPredictions].sort((a, b) => a - b);
        const binEdges: number[] = [];
        
        for (let i = 0; i <= bins; i++) {
            const quantile = i / bins;
            const index = Math.floor(quantile * (oldSorted.length - 1));
            binEdges.push(oldSorted[index]);
        }
        
        // Count observations in each bin
        const oldCounts = new Array(bins).fill(0);
        const recentCounts = new Array(bins).fill(0);
        
        oldPredictions.forEach(val => {
            for (let i = 0; i < bins; i++) {
                if (val >= binEdges[i] && val < binEdges[i + 1]) {
                    oldCounts[i]++;
                    break;
                }
            }
        });
        
        recentPredictions.forEach(val => {
            for (let i = 0; i < bins; i++) {
                if (val >= binEdges[i] && val < binEdges[i + 1]) {
                    recentCounts[i]++;
                    break;
                }
            }
        });
        
        // Calculate PSI
        let psi = 0;
        const oldTotal = oldPredictions.length;
        const recentTotal = recentPredictions.length;
        
        for (let i = 0; i < bins; i++) {
            const oldPct = (oldCounts[i] + 1e-10) / oldTotal; // Add smoothing
            const recentPct = (recentCounts[i] + 1e-10) / recentTotal;
            
            psi += (recentPct - oldPct) * Math.log(recentPct / oldPct);
        }
        
        return Math.max(0, psi);
    }
    
    private async collectSystemMetrics(): Promise<void> {
        const metrics: MLPerformanceMetrics = {
            timestamp: Date.now(),
            inferenceLatencyMs: this.getAverageLatency(),
            memoryUsageMB: this.getMemoryUsage(),
            throughputPredictionsPerSecond: this.getThroughput(),
            predictionConfidence: this.getAverageConfidence(),
            modelAccuracy: this.getAverageAccuracy(),
            cpuUtilization: await this.getCPUUtilization(),
            gpuUtilization: await this.getGPUUtilization()
        };
        
        this.metricsHistory.push(metrics);
        
        // Maintain history size
        if (this.metricsHistory.length > 1000) {
            this.metricsHistory = this.metricsHistory.slice(-500);
        }
    }
    
    private getAverageLatency(): number {
        if (this.rollingAverages.latency.length === 0) return 0;
        return this.rollingAverages.latency.reduce((a, b) => a + b, 0) / this.rollingAverages.latency.length;
    }
    
    private getAverageConfidence(): number {
        if (this.rollingAverages.confidence.length === 0) return 0;
        return this.rollingAverages.confidence.reduce((a, b) => a + b, 0) / this.rollingAverages.confidence.length;
    }
    
    private getAverageAccuracy(): number | undefined {
        if (this.rollingAverages.accuracy.length === 0) return undefined;
        return this.rollingAverages.accuracy.reduce((a, b) => a + b, 0) / this.rollingAverages.accuracy.length;
    }
    
    private getThroughput(): number {
        const recentBatches = this.predictionBatches.slice(-10);
        if (recentBatches.length === 0) return 0;
        
        const totalPredictions = recentBatches.reduce((sum, batch) => sum + batch.predictions.length, 0);
        const totalTimeMs = recentBatches.reduce((sum, batch) => sum + batch.latencyMs, 0);
        
        return totalTimeMs > 0 ? (totalPredictions / totalTimeMs) * 1000 : 0;
    }
    
    private getMemoryUsage(): number {
        const memInfo = tf.memory();
        return memInfo.numBytes / (1024 * 1024); // Convert to MB
    }
    
    private async getCPUUtilization(): Promise<number> {
        // Simplified CPU utilization calculation
        // In production, use more sophisticated monitoring
        return Math.random() * 30 + 10; // Mock 10-40% CPU usage
    }
    
    private async getGPUUtilization(): Promise<number> {
        // GPU utilization for Node.js backend
        // In production, integrate with nvidia-ml or similar
        return 0; // CPU backend doesn't use GPU
    }
    
    private performHealthCheck(): void {
        const recentMetrics = this.metricsHistory.slice(-10);
        if (recentMetrics.length === 0) return;
        
        const avgLatency = recentMetrics.reduce((sum, m) => sum + m.inferenceLatencyMs, 0) / recentMetrics.length;
        const avgConfidence = recentMetrics.reduce((sum, m) => sum + m.predictionConfidence, 0) / recentMetrics.length;
        const avgMemory = recentMetrics.reduce((sum, m) => sum + m.memoryUsageMB, 0) / recentMetrics.length;
        
        const healthStatus = {
            timestamp: Date.now(),
            latencyHealth: avgLatency < this.alertThresholds.maxLatencyMs ? 'HEALTHY' : 'WARNING',
            confidenceHealth: avgConfidence > this.alertThresholds.minConfidence ? 'HEALTHY' : 'WARNING',
            memoryHealth: avgMemory < this.alertThresholds.maxMemoryMB ? 'HEALTHY' : 'WARNING',
            overallHealth: 'HEALTHY' as 'HEALTHY' | 'WARNING' | 'CRITICAL'
        };
        
        // Determine overall health
        const warnings = Object.values(healthStatus).filter(status => status === 'WARNING').length;
        if (warnings >= 2) {
            healthStatus.overallHealth = 'CRITICAL';
        } else if (warnings >= 1) {
            healthStatus.overallHealth = 'WARNING';
        }
        
        this.emit('health-check', healthStatus);
        
        if (healthStatus.overallHealth !== 'HEALTHY') {
            console.log(`[WARN] üè• ML Health Check: ${healthStatus.overallHealth} - Latency: ${avgLatency.toFixed(2)}ms, Confidence: ${avgConfidence.toFixed(3)}, Memory: ${avgMemory.toFixed(2)}MB`);
        }
    }
    
    public getPerformanceReport(): {
        summary: any;
        recentMetrics: MLPerformanceMetrics[];
        driftAnalysis: ModelDriftMetrics[];
        benchmarkComparison: any[];
    } {
        const recentMetrics = this.metricsHistory.slice(-50);
        
        const summary = {
            totalInferences: this.predictionBatches.length,
            averageLatencyMs: this.getAverageLatency(),
            averageConfidence: this.getAverageConfidence(),
            averageAccuracy: this.getAverageAccuracy(),
            currentMemoryMB: this.getMemoryUsage(),
            throughputPPS: this.getThroughput(),
            activeInferences: this.activeInferences.size,
            lastDriftCheck: this.driftHistory.length > 0 ? this.driftHistory[this.driftHistory.length - 1] : null
        };
        
        const benchmarkComparison = Array.from(this.benchmarks.entries()).map(([name, benchmark]) => ({
            modelName: name,
            latencyVsTarget: (this.getAverageLatency() / benchmark.targetLatencyMs) * 100,
            memoryVsTarget: (this.getMemoryUsage() / benchmark.targetMemoryMB) * 100,
            accuracyVsTarget: this.getAverageAccuracy() ? (this.getAverageAccuracy()! / benchmark.targetAccuracy) * 100 : null
        }));
        
        return {
            summary,
            recentMetrics,
            driftAnalysis: this.driftHistory.slice(-10),
            benchmarkComparison
        };
    }
    
    public getInstanceId(): string {
        return this.instanceId;
    }
    
    public dispose(): void {
        this.removeAllListeners();
        this.metricsHistory = [];
        this.driftHistory = [];
        this.predictionBatches = [];
        this.activeInferences.clear();
        console.log(`[INFO] üßπ Enterprise ML Performance Monitor disposed: ${this.instanceId}`);
    }
}

export default EnterpriseMLPerformanceMonitor;
